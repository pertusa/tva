
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
      
      
      
      
      
      <link rel="icon" href="assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.6.1, mkdocs-material-9.6.16">
    
    
      
        <title>Machine learning methods for computer vision - TVA</title>
      
    
    
      <link rel="stylesheet" href="assets/stylesheets/main.7e37652d.min.css">
      
        
        <link rel="stylesheet" href="assets/stylesheets/palette.06af60db.min.css">
      
      


    
    
      
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
      
    
    
    <script>__md_scope=new URL(".",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
    
  </head>
  
  
    
    
    
    
    
    <body dir="ltr" data-md-color-scheme="default" data-md-color-primary="Teal" data-md-color-accent="Teal">
  
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#machine-learning-methods-for-computer-vision" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

  

<header class="md-header md-header--shadow" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href="." title="TVA" class="md-header__button md-logo" aria-label="TVA" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg>

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            TVA
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              Machine learning methods for computer vision
            
          </span>
        </div>
      </div>
    </div>
    
      
    
    
    
    
      
      
        <label class="md-header__button md-icon" for="__search">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        </label>
        <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
        <button type="reset" class="md-search__icon md-icon" title="Clear" aria-label="Clear" tabindex="-1">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"/></svg>
        </button>
      </nav>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" tabindex="0" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list" role="presentation"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
      
    
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    



<nav class="md-nav md-nav--primary" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href="." title="TVA" class="md-nav__button md-logo" aria-label="TVA" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg>

    </a>
    TVA
  </label>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
  
  
    
  
  
  
    <li class="md-nav__item md-nav__item--active">
      
      <input class="md-nav__toggle md-toggle" type="checkbox" id="__toc">
      
      
        
      
      
        <label class="md-nav__link md-nav__link--active" for="__toc">
          
  
  
  <span class="md-ellipsis">
    Machine learning methods for computer vision
    
  </span>
  

          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <a href="mltva.html" class="md-nav__link md-nav__link--active">
        
  
  
  <span class="md-ellipsis">
    Machine learning methods for computer vision
    
  </span>
  

      </a>
      
        

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#bag-of-features" class="md-nav__link">
    <span class="md-ellipsis">
      Bag of Features
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#face-detection" class="md-nav__link">
    <span class="md-ellipsis">
      Face detection
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Face detection">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#viola-jones" class="md-nav__link">
    <span class="md-ellipsis">
      Viola-Jones
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#face-recognition" class="md-nav__link">
    <span class="md-ellipsis">
      Face recognition
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Face recognition">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#eigenfaces" class="md-nav__link">
    <span class="md-ellipsis">
      Eigenfaces
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
    </ul>
  
</nav>
      
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#bag-of-features" class="md-nav__link">
    <span class="md-ellipsis">
      Bag of Features
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#face-detection" class="md-nav__link">
    <span class="md-ellipsis">
      Face detection
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Face detection">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#viola-jones" class="md-nav__link">
    <span class="md-ellipsis">
      Viola-Jones
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#face-recognition" class="md-nav__link">
    <span class="md-ellipsis">
      Face recognition
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Face recognition">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#eigenfaces" class="md-nav__link">
    <span class="md-ellipsis">
      Eigenfaces
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          
            <div class="md-content" data-md-component="content">
              <article class="md-content__inner md-typeset">
                
                  



<h1 id="machine-learning-methods-for-computer-vision">Machine Learning methods for Computer Vision<a class="headerlink" href="#machine-learning-methods-for-computer-vision" title="Permanent link">&para;</a></h1>
<p>As we have seen, before the deep learning era most computer vision methods used handcrafted features. The difficulty of selecting the most suitable features for a vision task has hindered the reliability of such methods for image detection, classification, and segmentation.</p>
<p>Nonetheless, some traditional machine learning methods have been successfully applied to computer vision. In this unit, we will examine some of them, including dimensionality reduction (particularly, Bag of Words applied to local descriptors), and face detection and recognition (Viola-Jones and EigenFaces), which have been successfully applied in the literature.</p>
<h2 id="bag-of-features">Bag of Features<a class="headerlink" href="#bag-of-features" title="Permanent link">&para;</a></h2>
<p>The Bag of Features (BoF) method, also known as Bag of Words (BoW), can be reduce the dimensionality of local descriptors.  BoW represents image features, and is inspired by the bag-of-words model often used in the context of Natural Language Processing, hence its name. </p>
<p>This section explains the BOW method formally.</p>
<p>Let's consider a training dataset <script type="math/tex">ùê∑=\{ùë•_1,‚Ä¶,ùë•_ùëÅ\}</script> of <script type="math/tex">ùëÅ</script> training images. Note that an image <script type="math/tex">ùë•_ùëñ\in ùê∑</script> may contain a different number of features (keypoints and descriptors) than another image. One of the main problems of SIFT or SURF is that performing a similarity search between two images implies matching all points from one image <script type="math/tex">x_i</script> with the points of another image <script type="math/tex">x_j</script>, which is a very costly procedure with a complexity <script type="math/tex">O(NM)</script>, being <script type="math/tex">N</script> and <script type="math/tex">M</script> the number of keypoints of <script type="math/tex">x_i</script> and <script type="math/tex">x_j</script>, respectively.  </p>
<p>BoW extracts a single feature vector of fixed size <script type="math/tex">ùëò</script> for any image independently of its number of keypoints. </p>
<!-- Explicaci√≥n: https://ai.stackexchange.com/questions/21914/what-are-bag-of-features-in-computer-vision -->

<p>The BoW process can be divided into three steps:</p>
<ol>
<li>
<p><strong>Feature extraction</strong> 
   First, we extract the features (i.e., keypoints and descriptors) from all images in the training dataset <script type="math/tex">ùê∑</script>. This can be done, for example, using SIFT. Let <script type="math/tex">ùêπ=\{ùëì_1,‚Ä¶,ùëì_ùëÄ\}</script> be the set of descriptors extracted from all training images in <script type="math/tex">ùê∑</script>. So, <script type="math/tex">ùëì_ùëñ</script> may be a descriptor that belongs to any of the training examples (it is not stored to which one).</p>
</li>
<li>
<p><strong>Codebook generation</strong>.
   In this step, we cluster all descriptors <script type="math/tex">ùêπ=\{ùëì_1,‚Ä¶,ùëì_ùëÄ\}</script> into <script type="math/tex">ùëò</script> clusters using <script type="math/tex">k</script>-means (or another clustering algorithm. Therefore, we have <script type="math/tex">ùëò</script> clusters, each of them with a centroid <script type="math/tex">ùê∂=\{ùëê_1,‚Ä¶,ùëê_ùëò\}</script>. These centroids represent the main features present in the whole training dataset <script type="math/tex">ùê∑</script>. In this context, they are often known as codewords or visual words (hence the name bag-of-visual-words). The set of codewords <script type="math/tex">ùê∂</script> is often called a codebook or vocabulary.</p>
</li>
<li>
<p><strong>Feature vector generation</strong>.
   In this last step, given a new image <script type="math/tex">ùë¢‚àâùê∑</script>, we  represent <script type="math/tex">ùë¢</script> as a <script type="math/tex">ùëò</script>-dimensional vector (where <script type="math/tex">ùëò</script> is the number of codewords). To do that, we need to follow the following steps: 
   1) Extract the raw features from <script type="math/tex">ùë¢</script> with e.g. SIFT (as we did for the training images). Let the descriptors of <script type="math/tex">ùë¢</script> be <script type="math/tex">ùëà=\{ùë¢_1,‚Ä¶,ùë¢_{|ùëà|}\}</script>. 
   2) Create a vector <script type="math/tex">ùêº</script> of size <script type="math/tex">ùëò</script> filled with zeros, where the <script type="math/tex">ùëñ</script>th element of <script type="math/tex">ùêº</script> corresponds to the <script type="math/tex">ùëñ</script>th codeword (or cluster).
   3) For each descriptor <script type="math/tex">ùë¢_ùëñ</script>, find the closest codeword (or centroid) in <script type="math/tex">ùê∂</script>. Once found, increment the value at the <script type="math/tex">ùëó</script>th position of <script type="math/tex">ùêº</script> (i.e., initially, from zero to one), where <script type="math/tex">ùëó</script> is the closest codeword to the descriptor <script type="math/tex">ùë¢_ùëñ</script> of the query image. The distance between <script type="math/tex">ùë¢_ùëñ</script> and any of the codewords can be computed with the Euclidean distance, for example. </p>
</li>
</ol>
<p>At the end of this process, we obtain a feature vector <script type="math/tex">ùêº</script> of size <script type="math/tex">k</script> that represents a <strong>histogram of codewords</strong> for the query image. Here is an illustrative example of such a histogram:</p>
<p><img alt="BOW" src="images/bow.jpg" /></p>
<p>As can be seen, an image can be represented by a histogram of codewords. In this example, for the sake of clarity there are only 4 codewords.</p>
<p>Alternatively, we can use the tf-idf rather than the codeword frequency. In that case, instead of representing each image with a vector containing the frequency of the codewords, it is represented with the frequency of the codewords weighted by their presence in other images. </p>
<!---
###¬†Implementation in OpenCV

With the following code, we can train a BoW codebook from the SIFT descriptors extracted from all the images in a training set:

<div class="highlight"><pre><span></span><code><span class="c1"># We create a BOW instance, in this case the vocabulary will have 100 codewords</span>
<span class="n">BOW</span> <span class="o">=</span> <span class="n">cv</span><span class="o">.</span><span class="n">BOWKMeansTrainer</span><span class="p">(</span><span class="mi">100</span><span class="p">)</span>
<span class="n">sift</span> <span class="o">=</span> <span class="n">cv</span><span class="o">.</span><span class="n">SIFT_create</span><span class="p">()</span>

<span class="c1"># We go through all the images extracting SIFT descriptors and adding them so we can train our BOW.</span>
<span class="k">for</span> <span class="n">file</span> <span class="ow">in</span> <span class="n">imagesPath</span><span class="p">:</span>
     <span class="n">image</span> <span class="o">=</span> <span class="n">cv</span><span class="o">.</span><span class="n">imread</span><span class="p">(</span><span class="n">file</span><span class="p">,</span> <span class="n">cv</span><span class="o">.</span><span class="n">IMREAD_GRAYSCALE</span><span class="p">)</span>
     <span class="n">keypoints</span><span class="p">,</span> <span class="n">descriptors</span> <span class="o">=</span> <span class="n">sift</span><span class="o">.</span><span class="n">detectAndCompute</span><span class="p">(</span><span class="n">image</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
     <span class="n">BOW</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">descriptors</span><span class="p">)</span>

<span class="c1"># We train the BoW to obtain the codebook (vocabulary)</span>
<span class="n">vocabulary</span> <span class="o">=</span> <span class="n">BOW</span><span class="o">.</span><span class="n">cluster</span><span class="p">()</span>
</code></pre></div>

Note that you must include the path to one or more images in the `imagesPath` variable in order to run the code. 

Once the code is executed, we obtain a trained dictionary of `k=100` words. Next we can extract a descriptor and convert it into a histogram of codewords (this will be our new descriptor for that image).

<div class="highlight"><pre><span></span><code><span class="c1"># We initialize the extractor, which will be based on SIFT and which will assign clusters by brute force</span>
<span class="n">BOWExtractor</span> <span class="o">=</span> <span class="n">cv</span><span class="o">.</span><span class="n">BOWImgDescriptorExtractor</span><span class="p">(</span><span class="n">sift</span><span class="p">,</span> <span class="n">cv</span><span class="o">.</span><span class="n">BFMatcher</span><span class="p">(</span><span class="n">cv</span><span class="o">.</span><span class="n">NORM_L2</span><span class="p">))</span>

<span class="c1"># We assign to the declared extractor the vocabulary we had trained</span>
<span class="n">BOWExtractor</span><span class="o">.</span><span class="n">setVocabulary</span><span class="p">(</span><span class="n">vocabulary</span><span class="p">)</span>

<span class="c1"># Now we can extract the BoW feature of an image</span>
<span class="n">BOWdescriptor</span> <span class="o">=</span> <span class="n">BOWExtractor</span><span class="o">.</span><span class="n">compute</span><span class="p">(</span><span class="n">image</span><span class="p">,</span> <span class="n">sift</span><span class="o">.</span><span class="n">detect</span><span class="p">(</span><span class="n">image</span><span class="p">))</span>

<span class="c1"># We show the descriptor</span>
<span class="n">plt</span><span class="o">.</span><span class="n">hist</span><span class="p">(</span><span class="n">BOWdescriptor</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="mi">100</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</code></pre></div>

You can see complete code examples for training and recognition with BOW [here](https://github.com/briansrls/SIFTBOW/blob/master/SIFTBOW.py) and also [here](https://github.com/mgmacias95/Flower-Recognition/blob/master/flower.py).
--->

<h2 id="face-detection">Face detection<a class="headerlink" href="#face-detection" title="Permanent link">&para;</a></h2>
<p>Identifying human faces in digital images is one of the few computer vision tasks that has been succesfully approached using machine learning. </p>
<p><img alt="Face detection. Source: https://en.wikipedia.org/wiki/Face_detection#/media/File:Face_detection.jpg" src="images/Face_detection.jpg" /></p>
<p>Face detection can be considered as a specific type of object detection. The goal of object detection methods is to find the bounding boxes (locations and sizes) of specific objects in an image. </p>
<!--
https://www.baeldung.com/cs/viola-jones-algorithm#:~:text=Viola%2DJones%20algorithm%20is%20a,primarily%20conceived%20for%20face%20detection.

https://en.wikipedia.org/wiki/Viola‚ÄìJones_object_detection_framework

https://www.cs.ubc.ca/~lowe/425/slides/13-ViolaJones.pdf

-->

<h3 id="viola-jones">Viola-Jones<a class="headerlink" href="#viola-jones" title="Permanent link">&para;</a></h3>
<p>The Viola-Jones algorithm, developed by Paul Viola and Michael Jones in 2001, is a pioneering technique in computer vision for real-time face detection. It's recognized for its speed and accuracy, making it particularly suitable for applications like surveillance and web cameras. The algorithm was groundbreaking for its time, providing a robust and efficient method for real-time face detection, and its principles are still used and built upon in more modern face detection technologies.</p>
<p>The full paper is <a href="https://www.cs.cmu.edu/~efros/courses/LBMV07/Papers/viola-cvpr-01.pdf">in this link</a>, but here we are going to see a summary of its key components described in detail below:</p>
<ol>
<li>
<p><strong>Haar Features:</strong>
   The algorithm uses Haar-like features, which are simple rectangular patterns. These features capture the presence of oriented contrasts between different regions of an image. For example, a feature might focus on the intensity difference between the eye region and the cheek region in a face. </p>
</li>
<li>
<p><strong>Integral image</strong>
   The Haar features are efficiently calculated using the integral image. This representation of an image allows for the rapid calculation of the sum of pixel values in any rectangular area. In this representation, each point <script type="math/tex">(x, y)</script> in the integral image contains the sum of the pixel values above and to the left of <script type="math/tex">(x, y)</script>, inclusive.</p>
</li>
<li>
<p><strong>Adaptive Boosting (AdaBoost):</strong>
   AdaBoost is a supervised machine learning method based on boosting. In the context of the Viola-Jones algorithm, it is used to select a small number of important features from a larger set, and also to train classifiers that use these features. </p>
</li>
<li>
<p><strong>Attentional Cascade:</strong>
   The algorithm uses a cascade of classifiers to quickly discard non-facial regions in an image, thereby reducing the computational burden. Each stage of the cascade is a classifier made up of a combination of several features. Early stages use fewer features for a rough filter, effectively discarding large areas of the image where a face is unlikely to be found. Subsequent stages use more features for a finer analysis.</p>
</li>
</ol>
<!--
5. **Scaling for Multi-size Face Detection:**
   To detect faces of various sizes, the Viola-Jones algorithm scales the detection window rather than the image, efficiently searching for faces across different scales.
-->

<p>Now, let's dive into these components:</p>
<p><strong>1. Haar features</strong></p>
<p>The Viola-Jones algorithm uses a set of features similar to Haar wavelets, which are a set of square-shaped functions. More specifically, the algorithm uses three types of Haar-like features represented in the following figure:</p>
<p><img alt="Haar features" src="images/haar.png" /></p>
<p>In these boxes, white represents 1, and black is -1. Therefore, when convolving a feature with a part of the image, the sum of the pixels which lie within the white rectangles are subtracted from the sum of pixels in the black rectangles.</p>
<p>The feature value will be around zero for ‚Äúflat regions‚Äù where all the pixels have the same value. However, a large feature value will be obtained in regions where the pixels in the black and white rectangles are very different.</p>
<p>As shown below, the following features are very important in face detection since the eye region is darker than the cheeks, and also darker than the nose region. </p>
<p><img alt="Haar features. Source: https://www.baeldung.com/cs/viola-jones-algorithm#:~:text=Viola%2DJones%20algorithm%20is%20a,primarily%20conceived%20for%20face%20detection." src="images/fig2.jpeg" /></p>
<p><strong>2. Integral image</strong></p>
<p>The Haar features can be computed on an image using convolutions, but since they are  "flat" (black or white), they can also be efficiently calculated using the integral image representation. Hence, the Haar-like features can be computed very quickly using this representation.</p>
<p>The integral image is calculated as follows. Given a grayscale image <script type="math/tex">I</script>, the integral image value <script type="math/tex">ii(x, y)</script> at the point <script type="math/tex">(x, y)</script> is the sum of all the pixels above and to the left of <script type="math/tex">(x, y)</script>, inclusive:</p>
<p>
<script type="math/tex; mode=display">ii(x, y)=\sum_{\substack{x^{\prime} \leq x \\ y^{\prime} \leq y}} I\left(x^{\prime}, y^{\prime}\right)</script>
</p>
<p>The integral image can be computed in a single pass over the image <script type="math/tex">I</script> with the following equation:</p>
<p>
<script type="math/tex; mode=display">ii(x, y)=I(x, y)+ii(x, y-1)+ii(x-1, y)-ii(x-1, y-1)</script>
</p>
<p>Therefore, given an image with <script type="math/tex">N</script> pixels, the time complexity of the integral image computation is <script type="math/tex">O(N)</script>.</p>
<p>The sum in any rectangular area requires four values of the integral image, regardless of the window size:</p>
<p><img alt="Integral image. Source: https://www.baeldung.com/cs/viola-jones-algorithm#:~:text=Viola%2DJones%20algorithm%20is%20a,primarily%20conceived%20for%20face%20detection." src="images/fig3.jpeg" /></p>
<p>More specifically, the sum of pixel values within any rectangle <script type="math/tex">ABCD</script> of a Haar-like feature can be computed as:</p>
<p>
<script type="math/tex; mode=display">\sum_{\substack{x_0<x \leq x_1 \\ y_0<y \leq y_1}} I(x, y)=ii(D)+ii(A)-ii(B)-ii(C)</script>
</p>
<p>Therefore, the calculation of the feature value for a black or white region is straighforward and very efficient, requiring only three sums. </p>
<!--
COJONUDO EJERCICIO: https://github.com/varunjain3/EigenFaces#1.-Viola-Jones-Face-Detection
-->
<p><strong>3. Adaptive Boosting (AdaBoost)</strong></p>
<!--
Buen video Adaboost (idea): https://www.youtube.com/watch?v=hfSKfuUVu9I
Mejor explicaci√≥n: https://www.analyticsvidhya.com/blog/2021/03/introduction-to-adaboost-algorithm-with-python-implementation/

https://www.youtube.com/watch?v=O7J9Dl1cWmM
-->

<p>Adaptive boosting is a boosting ensemble technique that sequentially trains weak classifiers to form a strong classifier. </p>
<p>You can read <a href="https://www.analyticsvidhya.com/blog/2021/06/adaboost-a-brief-introduction-to-ensemble-learning/">this explanation</a> to get an overview of the AdaBoost algorithm. Essentially, at each iteration, a weak classifier is trained using as input the dataset and assigning higher weights to data points that were wrongly predicted by previous classifiers. </p>
<p><img alt="Adaboost. Source:https://www.analyticsvidhya.com/blog/2021/03/" src="images/adaboost.png" /></p>
<p>The AdaBoost algorithm has the following steps:</p>
<ol>
<li>Train a model and perform inference.</li>
<li>Assign higher weights to misclassified points.</li>
<li>Train next model.</li>
<li>Repeat steps 2 and 3.</li>
<li>Get a weighted average of individual models.</li>
</ol>
<p>Ultimately, a robust classifier is built with the weighted combination of the individual models.</p>
<p>In the context of Viola-Jones method, AdaBoost is used to select a small number of relevant visual features. Within any image sub-window, the total number of Haar-like features is very large (far larger than the number of pixels). In order to ensure fast classification, the learning process must exclude a large majority of the available features and focus on a small set of critical features. </p>
<p>Feature selection is achieved through a simple modification of AdaBoost: The weak learner is constrained so that each weak classifier returned depends only of a single feature. As a result, each stage of the boosting process (which selects a new weak classifier) can be seen as a feature selection process. </p>
<p>Viola-Jones uses feature selection to select the best 6,000 features, although the method already achieves an accuracy of 95% with the best 200 features.</p>
<p>The best two features chosen are shown in the previous figure (see 1. Haar features). They are focused on the detection of the eyes and the nose.</p>
<p><strong>4. Classifier Cascades</strong></p>
<p>The Viola-Jones method uses sliding windows on the image (originally of size 24x24) and, for each window, 6,000 features are selected to detect if there is a face on a given region. </p>
<p>Nonetheless, with 6,000 features the complexity is still too high for real-time processing, so the authors found another solution. Since most pixels in an image do not correspond to faces, they devised a method to quickly detect if a window had face pixels, and discard it if not.</p>
<p>To achieve this, Viola-Jones employed a cascade of classifiers, also trained with AdaBoost. Instead of applying the 6,000 features per window, the features were placed in different stages in a hierarchical manner (each with its own classifier). If in the first stage, the window returns that there is no face, it is discarded and the following ones are not considered. If it passes, the second stage is applied, and this continues until the final stage is reached (if it passes all stages, then it is a face).</p>
<p><img alt="Cascade of classifiers. Source: Viola-Jones paper" src="images/cascadehaar.jpg" /></p>
<p>The 6,000 features were separated into 38 stages with 1, 10, 25, 25 and 50 features in the first five stages, placing the best features first.</p>
<h2 id="face-recognition">Face recognition<a class="headerlink" href="#face-recognition" title="Permanent link">&para;</a></h2>
<p>A facial recognition system aims to match a human face against a dataset of faces. Such a system is typically employed to authenticate users through identity verification services, and works by pinpointing and measuring facial features from a given image.</p>
<p>Eigenfaces is a remarkable face recognition technique that has been widely used on devices before the deep learning emerged. We are going to describe it next. </p>
<h3 id="eigenfaces">Eigenfaces<a class="headerlink" href="#eigenfaces" title="Permanent link">&para;</a></h3>
<p><a href="https://sites.cs.ucsb.edu/~mturk/Papers/mturk-CVPR91.pdf">Eigenfaces</a> is a method used in computer vision and face recognition that involves a mathematical approach to process human faces. The concept can be broken down into several key points:</p>
<ol>
<li>
<p><strong>Principal Component Analysis (PCA)</strong>. This is the core of eigenfaces. It is a statistical method used to reduce the dimensionality of large datasets while preserving most of the variance in the data. PCA identifies the directions (principal components) in which the data varies the most.</p>
</li>
<li>
<p><strong>Application to faces</strong>. In the context of face recognition, each face image is converted into a vector of pixel values. These vectors form a high-dimensional dataset. PCA is applied to this dataset to identify the principal components. These components are essentially the eigenfaces, which are a set of standardised face images. Any face can be approximated by a combination of these eigenfaces, so each face in the dataset can be represented as a weighted sum of eigenfaces. The weights indicate how much each eigenface contributes to the particular face image.</p>
</li>
<li>
<p><strong>Face Recognition</strong>. During training, a dataset of faces is used to compute the eigenfaces and the weights for each known face. For recognition, the same process is applied to a new face image to obtain its weights. The new face is then compared to the known faces by measuring the similarity in their weights.</p>
</li>
</ol>
<p>Here you can see some examples of eigenfaces:</p>
<p><img alt="Eigenfaces. Source: https://en.wikipedia.org/wiki/Eigenface" src="images/Eigenfaces.png" /></p>
<p>Any human face can be considered to be a combination of these standard eigenfaces. For example, one's face might be composed of the average face plus 10% from eigenface 1.55% from eigenface 2, and even ‚àí3% from eigenface 3. Remarkably, it does not take many eigenfaces combined together to achieve a fair approximation of most faces. </p>
<p><strong>Methodology</strong></p>
<p>To better understand the eigenfaces method, in this section we follow the <a href="https://pyimagesearch.com/2021/05/10/opencv-eigenfaces-for-face-recognition/">explanation from Adrian Rosebrock</a> that is summarized next.</p>
<p>To train the eigenfaces algorithm, we need to form a single vector from the image. This is accomplished by flattening each image into a <script type="math/tex">K^2</script>-dim vector. All we have done here is taken a <script type="math/tex">K√óK</script> image and concatenated all of the rows together, forming a single and long <script type="math/tex">K^2</script> list of grayscale pixel intensities.</p>
<p><img alt="Eigenfaces flattening" src="images/eigenfaces_face_flatten.png" /></p>
<p>After each image in the dataset has been flattened, we form a matrix of flattened images, where <script type="math/tex">Z</script> is the total number of images in our dataset:</p>
<p><img alt="Eigenfaces training set matrix" src="images/eigenfaces_image_matrix.png" /></p>
<p>Our entire training set is now contained in a single matrix, <script type="math/tex">M</script>.</p>
<p>Given this matrix <script type="math/tex">M</script>, we are now ready to apply <strong>Principal Component Analysis (PCA)</strong>, the cornerstone of the Eigenfaces algorithm.</p>
<p>A complete review associated with the linear algebra underlying PCA is outside the scope of this lesson (for a detailed review of the algorithm, please see <a href="https://www.coursera.org/learn/machine-learning">Andrew Ng‚Äôs discussion on the topic</a>), but the general outline of the algorithm follows:</p>
<ol>
<li>Compute the mean <script type="math/tex">\mu_{i}</script> of each column in the matrix, giving us the average pixel intensity value for every <script type="math/tex">(x, y)</script>-coordinate in the image dataset.</li>
<li>Subtract the <script type="math/tex">\mu_{i}</script> from each column <script type="math/tex">c_{i}</script>. This is called mean centering the data and is a required step when performing PCA.</li>
<li>Now that our matrix <script type="math/tex">M</script> has been mean centered, compute the covariance matrix.</li>
<li>Perform an eigenvalue decomposition on the covariance matrix to get the eigenvalues <script type="math/tex">\lambda_{i}</script> and eigenvectors <script type="math/tex">\mathbf{X_{i}}</script>.</li>
<li>Sort <script type="math/tex">\mathbf{X_{i}}</script> by <script type="math/tex">|\lambda_{i}|</script>, largest to smallest.</li>
<li>Take the top <script type="math/tex">N</script> eigenvectors with the largest corresponding eigenvalue magnitude.</li>
<li>Transform the input data by projecting (i.e., taking the dot product) it onto the space created by the top <script type="math/tex">N</script> eigenvectors. <strong>These eigenvectors are called our eigenfaces.</strong></li>
</ol>
<p>Before we perform actual face identification using the Eigenfaces algorithm, let‚Äôs actually discuss these eigenface representations:</p>
<p><img alt="Eigenfaces matrix" src="images/eigenfaces_decomp_matrix.png" /></p>
<p>After applying an eigenvalue decomposition to the matrix, <script type="math/tex">M</script>, we are left with a matrix <script type="math/tex">V</script>, containing <script type="math/tex">N</script> rows (our eigenvectors) each of dimensionality <script type="math/tex">K^{2}</script>.</p>
<p>Each row in the matrix above is an eigenface with <script type="math/tex">K^{2}</script> entries ‚Äî exactly like our original image.</p>
<p>What does this mean? Well, since each of these eigenface representations is actually a <script type="math/tex">K^{2}</script> vector, we can reshape it into a <script type="math/tex">K\times K</script> bitmap:</p>
<p><img alt="Eigenfaces mean and components" src="images/eigenfaces_mean_and_components.png" /></p>
<p>The image on the left is simply the average of all faces in our dataset, while the figures on the right show the most prominent deviations from the mean in our face dataset.</p>
<p>This can be thought of as a visualization of the dimension in which people‚Äôs faces differ the most. Lighter regions indicate greater variation, where darker regions correspond to little to no variation. Here, we can see that our eigenface representation captures considerable variance in the eyes, hair, nose, lips, and cheek structure.</p>
<p><strong>Identifying faces using Eigenfaces</strong></p>
<p>Given our eigenface vectors, we can represent a new face by taking the dot product between the (flattened) input face image and the <script type="math/tex">N</script> eigenfaces. This allows us to represent each face as a linear combination of principal components:</p>
<p>Query Face = 36% of Eigenface #1 + -8% of Eigenface #2 + ‚Ä¶ + 21% of Eigenface <script type="math/tex">N</script>
</p>
<p>To perform the actual face identification, Sirovich and Kirby proposed taking the Euclidean distance between projected eigenface representations. This is, in essence, a k-NN classifier:</p>
<p><img alt="Eigenfaces face distances" src="images/eigenfaces_face_distances.png" /></p>
<p>The smaller the Euclidean distance (denoted as the function <script type="math/tex">d</script>), the more similar the two faces are ‚Äî the overall identification is found by taking the label associated with the face with the smallest Euclidean distance.</p>
<p>For example, in the previous figure the top image pair has a distance of 0 because the two faces are identical (i.e., the same image).</p>
<p>The middle image pair has a distance of 0.07 ‚Äî while the images are different they contain the same face.</p>
<p>The third image pair has a much larger distance (9.81), indicating that the two faces presented to the Eigenfaces algorithm are not the same person.</p>
<p>In practice, we often do not rely on a simple <script type="math/tex">k</script>-NN algorithm for identification. Accuracy can be increased by using more advanced machine learning algorithms, such as Support Vector Machines (SVMs), Random Forests, etc. </p>
<p><strong>Discussion</strong></p>
<p>Eigenfaces was one of the first successful applications of PCA for face recognition and has stablished the foundation for more sophisticated methods in computer vision and pattern recognition.</p>
<p>Since a person's face is not represented by a digital photograph, but instead as a list of values (one for each eigenface in the database), significantly less space is needed for each face. Therefore, one of the main advantages of this method is efficiency: Reducing faces to weights on eigenfaces significantly reduces the amount of data to be processed. Another strength is that eigenfaces can generalize well to new faces not seen during training.</p>
<p>However, eigenfaces also have some limitations. The method may struggle with variations in lighting, facial expressions, and pose. Also, it relies on a good alignment of faces in the images.</p>
<!--

Recognition: EigenFaces (+ Fisherfaces?)

https://en.wikipedia.org/wiki/Eigenface

http://www.scholarpedia.org/article/Eigenfaces

Para ejercicio: https://github.com/varunjain3/EigenFaces/blob/master/EigenFace_Method.ipynb

-->

<p>Various extensions have been made to this approach. The <a href="https://ieeexplore.ieee.org/abstract/document/531802">eigenfeatures</a> method combines facial metrics (measuring distance between facial features) with the eigenface representation. <a href="https://cseweb.ucsd.edu/classes/wi14/cse152-a/fisherface-pami97.pdf">Fisherfaces</a> use linear discriminant analysis (LDA) and are less sensitive to variations in lighting and facial pose. </p>












                
              </article>
            </div>
          
          
<script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script>
        </div>
        
      </main>
      
        <footer class="md-footer">
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    
    
      
      <script id="__config" type="application/json">{"base": ".", "features": ["content.code.copy"], "search": "assets/javascripts/workers/search.d50fe291.min.js", "tags": null, "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}, "version": null}</script>
    
    
      <script src="assets/javascripts/bundle.50899def.min.js"></script>
      
        <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
      
    
  </body>
</html>