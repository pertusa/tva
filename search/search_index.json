{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"index.html","title":"Home","text":"<p>Theory contents for the TVA subject, \"Master en Inteligencia Artificial\" from the University of Alicante.</p>"},{"location":"mltva.html","title":"Machine Learning methods for Computer Vision","text":"<p>As we have seen, before the deep learning era most computer vision methods used handcrafted features. The difficulty of selecting the most suitable features for a vision task has hindered the reliability of such methods for image detection, classification, and segmentation.</p> <p>Nonetheless, some traditional machine learning methods have been successfully applied to computer vision. In this unit, we will examine some of them, including dimensionality reduction (particularly, Bag of Words applied to local descriptors), and face detection and recognition (Viola-Jones and EigenFaces), which have been successfully applied in the literature.</p>"},{"location":"mltva.html#bag-of-features","title":"Bag of Features","text":"<p>The Bag of Features (BoF) method, also known as Bag of Words (BoW), can be reduce the dimensionality of local descriptors.  BoW represents image features, and is inspired by the bag-of-words model often used in the context of Natural Language Processing, hence its name. </p> <p>This section explains the BOW method formally.</p> <p>Let's consider a training dataset  of  training images. Note that an image  may contain a different number of features (keypoints and descriptors) than another image. One of the main problems of SIFT or SURF is that performing a similarity search between two images implies matching all points from one image  with the points of other image , which is a very costly procedure with a complexity , being  and  the number of keypoints of  and , respectively. </p> <p>BoW extracts a single feature vector of fixed size  for any image independently of its number of keypoints. </p> <p>The BoW process can be divided into three steps:</p> <ol> <li> <p>Feature extraction     First, we extract the features (i.e., keypoints and descriptors) from all images in the training dataset . This can be done, for example, using SIFT. Let  be the set of descriptors extracted from all training images in . So,  may be a descriptor that belongs to any of the training examples (it is not stored to which one).</p> </li> <li> <p>Codebook generation.    In this step, we cluster all descriptors  into  clusters using -means (or another clustering algorithm. Therefore, we have  clusters, each of them with a centroid . These centroids represent the main features present in the whole training dataset . In this context, they are often known as codewords or visual words (hence the name bag-of-visual-words). The set of codewords  is often called a codebook or vocabulary.</p> </li> <li> <p>Feature vector generation.    In this last step, given a new image , we  represent  as a -dimensional vector (where  is the number of codewords). To do that, we need to follow the following steps:     1) Extract the raw features from  with e.g. SIFT (as we did for the training images). Let the descriptors of  be .     2) Create a vector  of size  filled with zeros, where the th element of  corresponds to the th codeword (or cluster).    3) For each descriptor , find the closest codeword (or centroid) in . Once found, increment the value at the th position of  (i.e., initially, from zero to one), where  is the closest codeword to the descriptor  of the query image. The distance between  and any of the codewords can be computed with the Euclidean distance, for example. </p> </li> </ol> <p>At the end of this process, we obtain a feature vector  of size  that represents a histogram of codewords for the query image. Here is an illustrative example of such a histogram:</p> <p></p> <p>As can be seen, an image can be represented by a histogram of codewords. In this example, for the sake of clarity there are only 4 codewords.</p> <p>Alternatively, we can use the tf-idf rather than the codeword frequency. In that case, instead of representing each image with a vector containing the frequency of the codewords, it is represented with the frequency of the codewords weighted by their presence in other images. </p>"},{"location":"mltva.html#face-detection","title":"Face detection","text":"<p>Identifying human faces in digital images is one of the few computer vision tasks that has been succesfully approached using machine learning. </p> <p></p> <p>Face detection can be considered as a specific type of object detection. The goal of object detection methods is to find the bounding boxes (locations and sizes) of specific objects in an image. </p>"},{"location":"mltva.html#viola-jones","title":"Viola-Jones","text":"<p>The Viola-Jones algorithm, developed by Paul Viola and Michael Jones in 2001, is a pioneering technique in computer vision for real-time face detection. It's recognized for its speed and accuracy, making it particularly suitable for applications like surveillance and web cameras. The algorithm was groundbreaking for its time, providing a robust and efficient method for real-time face detection, and its principles are still used and built upon in more modern face detection technologies.</p> <p>The full paper is in this link, but here we are going to see a summary of its key components, that will be described in detail below:</p> <ol> <li> <p>Haar Features:    The algorithm uses Haar-like features, which are simple rectangular patterns. These features capture the presence of oriented contrasts between different regions of an image. For example, a feature might focus on the intensity difference between the eye region and the cheek region in a face. </p> </li> <li> <p>Integral image    The Haar features are efficiently calculated using the integral image. This representation of an image allows for the rapid calculation of the sum of pixel values in any rectangular area. In this representation, each point  in the integral image contains the sum of the pixel values above and to the left of , inclusive.</p> </li> <li> <p>Adaptive Boosting (AdaBoost):    AdaBoost is a supervised machine learning method based on boosting. In the context of the Viola-Jones algorithm, it is used to select a small number of important features from a larger set, and also to train classifiers that use these features. </p> </li> <li> <p>Attentional Cascade:    The algorithm uses a cascade of classifiers to quickly discard non-facial regions in an image, thereby reducing the computational burden. Each stage of the cascade is a classifier made up of a combination of several features. Early stages use fewer features for a rough filter, effectively discarding large areas of the image where a face is unlikely to be found. Subsequent stages use more features for a finer analysis.</p> </li> </ol> <p>Now, let's dive into these components:</p> <p>1. Haar features</p> <p>The Viola-Jones algorithm uses a set of features similar to Haar wavelets, which are a set of square-shaped functions. More specifically, the algorithm uses three types of Haar-like features represented in the following figure:</p> <p></p> <p>In these boxes, white represents 1, and black is -1. Therefore, when convolving a feature with a part of the image, the sum of the pixels which lie within the white rectangles are subtracted from the sum of pixels in the black rectangles.</p> <p>The feature value will be around zero for \u201cflat regions\u201d where all the pixels have the same value. However, a large feature value will be obtained in regions where the pixels in the black and white rectangles are very different.</p> <p>As shown below, the following features are very important in face detection since the eye region is darker than the cheeks, and also darker than the nose region. </p> <p></p> <p>2. Integral image</p> <p>The Haar features can be computed on an image using convolutions, but since they are  \"flat\" (black or white), they can also be efficiently calculated using the integral image representation. Hence, the Haar-like features can be computed very quickly using this representation.</p> <p>The integral image is calculated as follows. Given a grayscale image , the integral image value  at the point  is the sum of all the pixels above and to the left of , inclusive:</p> <p> </p> <p>The integral image can be computed in a single pass over the image  with the following equation:</p> <p> </p> <p>Therefore, given an image with  pixels, the time complexity of the integral image computation is .</p> <p>The sum in any rectangular area requires four values of the integral image, regardless of the window size:</p> <p></p> <p>More specifically, the sum of pixel values within any rectangle  of a Haar-like feature can be computed as:</p> <p> </p> <p>Therefore, the calculation of the feature value for a black or white region is straighforward and very efficient, requiring only three sums. </p> <p>3. Adaptive Boosting (AdaBoost)</p> <p>Adaptive boosting is a boosting ensemble technique that sequentially trains weak classifiers to form a strong classifier. </p> <p>You can view this explanation to get an overview of the AdaBoost algorithm. Essentially, at each iteration, a weak classifier is trained using as input the dataset and assigning higher weights to data points that were wrongly predicted by previous classifiers. </p> <p></p> <p>The AdaBoost algorithm has the following steps: 1. Train a model and perform inference 2. Assign higher weights to misclassified points 3. Train next model 4. Repeat steps 2 and 3 5. Get a weighted average of individual models</p> <p>Ultimately, a robust classifier is built with the weighted combination of individual models.</p> <p>In the context of Viola-Jones method, AdaBoost is used to select a small number of relevant visual features. Within any image sub-window, the total number of Harr-like features is very large (far larger than the number of pixels). In order to ensure fast classification, the learning process must exclude a large majority of the available features, and focus on a small set of critical features. </p> <p>Feature selection is achieved through a simple modification of AdaBoost: The weak learner is constrained so that each weak classifier returned depends only of a single feature. As a result, each stage of the boosting process  (which selects a new weak classifier) can be seen as a feature selection process. </p> <p>Viola-Jones used feature selection to select the best 6,000 features, although the method already achieved an accuracy of 95% with the best 200 features.</p> <p>The best two features chosen are shown in the previous figure (see 1. Haar features). They are focused on the detection of the eyes and the nose.</p> <p>4. Classifier Cascades</p> <p>The Viola-Jones method uses sliding windows on the image (originally of size 24x24) and, for each window, 6,000 features are selected to detect if there is a face on a given region. </p> <p>Nonetheless, with 6,000 features the complexity is still too high for real-time processing, so the authors found another solution. Since most pixels in an image do not correspond to faces, they devised a method to quickly detect if a window had face pixels, and discard it if not.</p> <p>To achieve this, Viola-Jones employed a cascade of classifiers, also trained with AdaBoost. Instead of applying the 6,000 features per window, the features were placed in different stages in a hierarchical manner (each with its own classifier). If in the first stage, the window returns that there is no face, it is discarded and the following ones are not considered. If it passes, the second stage is applied, and this continues until the final stage is reached (if it passes all stages, then it is a face).</p> <p></p> <p>The 6,000 features were separated into 38 stages with 1, 10, 25, 25 and 50 features in the first five stages, placing the best features first.</p>"},{"location":"mltva.html#face-recognition","title":"Face recognition","text":"<p>A facial recognition system aims to match a human face against a dataset of faces. Such a system is typically employed to authenticate users through identity verification services, and works by pinpointing and measuring facial features from a given image.</p> <p>Eigenfaces is a remarkable face recognition technique that has been widely used on devices before the deep learning era. We are going to describe it next. </p>"},{"location":"mltva.html#eigenfaces","title":"Eigenfaces","text":"<p>Eigenfaces is a method used in computer vision and face recognition that involves a mathematical approach to process human faces. The concept can be broken down into several key points:</p> <ol> <li> <p>Principal Component Analysis (PCA). This is the core of eigenfaces. It is a statistical method used to reduce the dimensionality of large datasets while preserving most of the variance in the data. PCA identifies the directions (principal components) in which the data varies the most.</p> </li> <li> <p>Application to faces. In the context of face recognition, each face image is converted into a vector of pixel values. These vectors form a high-dimensional dataset. PCA is applied to this dataset to identify the principal components. These components are essentially the eigenfaces, which are a set of standardised face images. Any face can be approximated by a combination of these eigenfaces, so each face in the dataset can be represented as a weighted sum of eigenfaces. The weights indicate how much each eigenface contributes to the particular face image.</p> </li> <li> <p>Face Recognition. During training, a dataset of faces is used to compute the eigenfaces and the weights for each known face. For recognition, the same process is applied to a new face image to obtain its weights. The new face is then compared to the known faces by measuring the similarity in their weights.</p> </li> </ol> <p>Here you can see some examples of eigenfaces:</p> <p></p> <p>Any human face can be considered to be a combination of these standard eigenfaces. For example, one's face might be composed of the average face plus 10% from eigenface 1, 55% from eigenface 2, and even \u22123% from eigenface 3. Remarkably, it does not take many eigenfaces combined together to achieve a fair approximation of most faces. </p> <p>Methodology</p> <p>To better understand the eigenfaces method, in this section we follow the explanation from Adrian Rosebrock that is summarized next.</p> <p>To train the eigenfaces algorithm, we need to form a single vector from the image. This is accomplished by flattening each image into a -dim vector. All we have done here is taken a  image and concatenated all of the rows together, forming a single and long  list of grayscale pixel intensities.</p> <p></p> <p>After each image in the dataset has been flattened, we form a matrix of flattened images, where  is the total number of images in our dataset:</p> <p></p> <p>Our entire training set is now contained in a single matrix, .</p> <p>Given this matrix , we are now ready to apply Principal Component Analysis (PCA), the cornerstone of the Eigenfaces algorithm.</p> <p>A complete review associated with the linear algebra underlying PCA is outside the scope of this lesson (for a detailed review of the algorithm, please see Andrew Ng\u2019s discussion on the topic), but the general outline of the algorithm follows:</p> <ol> <li>Compute the mean  of each column in the matrix, giving us the average pixel intensity value for every -coordinate in the image dataset.</li> <li>Subtract the  from each column . This is called mean centering the data and is a required step when performing PCA.</li> <li>Now that our matrix  has been mean centered, compute the covariance matrix.</li> <li>Perform an eigenvalue decomposition on the covariance matrix to get the eigenvalues  and eigenvectors .</li> <li>Sort  by , largest to smallest.</li> <li>Take the top  eigenvectors with the largest corresponding eigenvalue magnitude.</li> <li>Transform the input data by projecting (i.e., taking the dot product) it onto the space created by the top  eigenvectors. These eigenvectors are called our eigenfaces.</li> </ol> <p>Before we perform actual face identification using the Eigenfaces algorithm, let\u2019s actually discuss these eigenface representations:</p> <p></p> <p>After applying an eigenvalue decomposition to the matrix, , we are left with a matrix , containing  rows (our eigenvectors) each of dimensionality .</p> <p>Each row in the matrix above is an eigenface with  entries \u2014 exactly like our original image.</p> <p>What does this mean? Well, since each of these eigenface representations is actually a  vector, we can reshape it into a  bitmap:</p> <p></p> <p>The image on the left is simply the average of all faces in our dataset, while the figures on the right show the most prominent deviations from the mean in our face dataset.</p> <p>This can be thought of as a visualization of the dimension in which people\u2019s faces differ the most. Lighter regions indicate greater variation, where darker regions correspond to little to no variation. Here, we can see that our eigenface representation captures considerable variance in the eyes, hair, nose, lips, and cheek structure.</p> <p>Identifying faces using Eigenfaces</p> <p>Given our eigenface vectors, we can represent a new face by taking the dot product between the (flattened) input face image and the  eigenfaces. This allows us to represent each face as a linear combination of principal components:</p> <p>Query Face = 36% of Eigenface #1 + -8% of Eigenface #2 + \u2026 + 21% of Eigenface  </p> <p>To perform the actual face identification, Sirovich and Kirby proposed taking the Euclidean distance between projected eigenface representations. This is, in essence, a k-NN classifier:</p> <p></p> <p>The smaller the Euclidean distance (denoted as the function ), the more similar the two faces are \u2014 the overall identification is found by taking the label associated with the face with the smallest Euclidean distance.</p> <p>For example, in the previous figure the top image pair has a distance of 0 because the two faces are identical (i.e., the same image).</p> <p>The middle image pair has a distance of 0.07 \u2014 while the images are different they contain the same face.</p> <p>The third image pair has a much larger distance (9.81), indicating that the two faces presented to the Eigenfaces algorithm are not the same person.</p> <p>In practice, we often do not rely on a simple k-NN algorithm for identification. Accuracy can be increased by using more advanced machine learning algorithms, such as Support Vector Machines (SVMs), Random Forests, etc. </p> <p>Discussion</p> <p>Eigenfaces was one of the first successful applications of PCA for face recognition and has stablished the foundation for more sophisticated methods in computer vision and pattern recognition.</p> <p>Since a person's face is not represented by a digital photograph, but instead as a list of values (one for each eigenface in the database), significantly less space is needed for each face. Therefore, one of the main advantages of this method is efficiency: Reducing faces to weights on eigenfaces significantly reduces the amount of data to be processed. Another strength is that eigenfaces can generalize well to new faces not seen during training.</p> <p>However, eigenfaces also have some limitations. The method may struggle with variations in lighting, facial expressions, and pose. Also, it relies on a good alignment of faces in the images.</p> <p>Various extensions have been made to this approach. The eigenfeatures method combines facial metrics (measuring distance between facial features) with the eigenface representation. Fisherfaces use linear discriminant analysis (LDA) and are less sensitive to variations in lighting and facial pose. </p>"}]}